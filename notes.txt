## 1. Essential context — what this tool is, what it is not

This tool exists to analyze the **quality and internal consistency of a GPX file**, and by extension the **logging behavior of the device or software** that produced it.

It does **not** assign a score.
It does **not** make predictions.
It does **not** derive conclusions.

The tool exposes **observable properties, constraints, and invariants** present in the GPX. Any interpretation beyond that is intentionally left to the reader.

This is an **observation-only instrument**. It is not meant to be repurposed for ranking, validation of performance, or downstream automation.

### What this tool is not

* Not a fitness or performance analysis tool
* Not a route difficulty estimator
* Not a GPS correction or smoothing utility
* Not a GPX repair tool
* Not a validator that decides whether a GPX is “good” or “bad”

If you are looking for corrected data, inferred intent, or summarized judgments, this tool is intentionally not designed for that.

### Intended inputs and constraints

This tool is best used with **raw GPX files**:

* recorded directly by a device or application
* not manually merged
* not resampled
* not smoothed or pre-processed

Merged or post-processed GPX files often introduce **artificial invariants** (uniform timestamps, reordered points, interpolated segments) that this system does not assume or correct for.

Some malformed or edited GPX files may still load, but the resulting audits should be treated with caution.

---

## 2. How the system behaves — pipeline guarantees and non-guarantees

Although often described as a “pipeline”, this system is **not a traditional processing pipeline**. It does not progressively clean, fix, or transform the GPX.

Most stages **do not mutate data**. They observe and audit.

### Ingestion and the only upstream mutation

The only upstream mutation happens at ingestion:

* points with invalid or non-finite coordinates are rejected immediately

Coordinates are treated as **fundamental**. Without valid spatial information, all other GPX attributes become meaningless.

After this step, the dataset is frozen.

### Timestamp audit philosophy

Timestamps are treated as **optional but informative** data.

The timestamp audit checks only for:

* presence
* parseability
* **local ordering violations**

Backtracking is flagged **only for consecutive point-to-point comparisons**.

Example:

```
90 → 70 → 80
```

The transition `90 → 70` is flagged.
No assumptions are made about which value is “correct” across the larger block.

This conservative approach avoids block-level inference and prefers **false negatives over false positives**.

Timestamps are never reordered, corrected, or interpolated. They are flagged, not mutated.

### Why the pipeline is not linearly mutated

All downstream audits — timestamp audit, sampling audit, and visualization — operate on the **same ingestion-filtered data**.

The timestamp audit does not produce corrected output, only observations. For this reason, the sampling audit intentionally reads from **ingestion output**, not from timestamp audit output.

This design prevents cascading assumptions and keeps each module epistemically isolated.

### Sampling audits and local computation

Sampling-related metrics (time deltas, distance deltas, joint pairs) are computed **locally and ephemerally**.

These computations:

* do not mutate the GPX
* do not write back corrected values
* exist only to support visualization

If timestamps do not show temporal progression, time-based deltas are disabled. In such cases, geometry-based distance deltas may still be computed.

---

## 3. Design and visualization notes — why these graphs, why these choices

This section is a deeper dive into **why the visualization looks the way it does**. It is not required reading to use the tool correctly.

### Why KDE instead of histograms

The first principle was to avoid **hard binning**.

Histogram bins introduce arbitrary boundaries. Two values on either side of a bin edge are treated as categorically different, while values far apart within the same bin are treated as identical.

For GPX sampling data, this is misleading.

Kernel Density Estimation (KDE) allows:

* a continuous view of distribution shape
* every data point to influence nearby regions
* no hard discontinuities introduced by bin edges

### Why Gaussian kernels and why scaling mattered

Gaussian kernels were chosen for their smoothness and simplicity. However, Gaussian kernels have **infinite support**, which introduces boundary bias when the data domain is strictly positive (as time and distance deltas are).

This problem was especially visible near the origin, where most real GPX data lives.

Instead of clipping or reflecting the distribution, the solution was to change the **representation space**.

### Why log-scaled axes

Raw GPX deltas span multiple orders of magnitude:

* frequent small deltas from regular sampling
* rare large deltas from pauses or logging gaps

Using a linear scale collapses most of the data near zero and exaggerates outliers.

By computing KDE in **log space**:

* structure near the origin becomes visible
* long tails remain representable
* no artificial truncation is introduced

Values are still displayed in linear units. Only spacing is logarithmic.

### Why controllable bandwidth

There is no single “correct” bandwidth.

Different questions require different smoothness:

* broad trends in sampling behavior
* sharp peaks indicating fixed sampling intervals
* subtle secondary modes from power-saving or state changes

Automatic rules (such as Silverman’s rule) assume stable, well-behaved distributions. Raw GPX data often violates these assumptions.

A controllable bandwidth makes this tradeoff explicit instead of implicit.

### Why these three plots

Each plot answers a different, non-overlapping question:

* **KDE plots** show distribution shape and dominant sampling scales
* **Rug plots** show where data exists, not how much exists at a point
* **Scatter plots** show joint time–distance behavior

The scatter plot is often the most diagnostic view. For human movement, a plausible GPX usually forms a continuous trail. Highly isolated points often indicate logging faults or GPS teleportation.

### What these plots do not do

* They do not prove correctness
* They do not validate activity quality
* They do not summarize performance

They make structure visible, and stop there.

---

### Final note

This tool is designed to make a GPX file **legible**, not authoritative.

It exposes structure, constraints, and failure modes, and deliberately preserves ambiguity.

Anything beyond that is left to human judgment.

